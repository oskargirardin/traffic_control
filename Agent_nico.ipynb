{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import time\n",
    "import sys\n",
    "import traffic_control_game\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import json\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import matplotlib\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import tiles3 as tc\n",
    "\n",
    "#is_ipython = 'inline' in matplotlib.get_backend()\n",
    "#if is_ipython:\n",
    "#    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None, string=None):\n",
    "    '''\n",
    "    Function to compute the time\n",
    "    start_time : starting time generated calling this function without arguments the first time\n",
    "    string: visualization purposes (task description)\n",
    "    '''\n",
    "    if not start_time:\n",
    "        start_time=datetime.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.datetime.now()-start_time).total_seconds(),3600)\n",
    "        tmin, tsec = divmod(temp_sec,60)\n",
    "        pr = \"for \" + str(string) + \" \" if string else \"\"\n",
    "        print(\"Execution time\", pr, \"is \", thour,\" h :\", tmin,' m :', round(tsec,2), \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to load and store dictionaries safely by saving converting them\n",
    "to json format. Used due to the long training times and the instability\n",
    "of Collab\n",
    "\"\"\"\n",
    "\n",
    "def save_json(q, filename):\n",
    "    with open(f'{filename}.json', 'w') as fp:\n",
    "        to_json = str({k: list(v) for k, v in q.items()})\n",
    "        json.dump(to_json, fp)\n",
    "    \n",
    "\n",
    "def load_json(filename):\n",
    "    with open(f'{filename}.json') as json_file:\n",
    "        data = ast.literal_eval(json.load(json_file))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations space: Dict('NS': Discrete(76), 'WE': Discrete(92), 'pa': Discrete(6), 'wt': Discrete(1500))\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Environment initialization\n",
    "\n",
    "env_steps = 50\n",
    "max_waiting_time = 1500\n",
    "ps_ns = np.random.uniform(low=0.09, high=0.1, size=1)\n",
    "ps_ew = np.random.uniform(low=0.04, high=0.05, size=1)\n",
    "ps = np.tile(np.concatenate((ps_ns, ps_ew)), 2)\n",
    "\n",
    "env_info = {\"ps\": ps, \"max_wait_time\": max_waiting_time, \"env_steps\": env_steps, \"n_states\": 2}        \n",
    "        \n",
    "env = gym.make(\"traffic_control-v0\", env_info=env_info, render_mode=\"human\") \n",
    "\n",
    "print(f\"Observations space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-gradient Sarsa agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TileCoder:\n",
    "    ''' Class to facilitate tile coding representations of states passed as parameters '''\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, queue_ns, queue_we, wt, pa):\n",
    "        global env, env_info\n",
    "        \n",
    "        # Range of minimum and maximum value for each of the 2 components of the observation vector\n",
    "        ns_min, ns_max = float(env.observation_space[\"NS\"].start), float(env.observation_space[\"NS\"].start + env.observation_space[\"NS\"].n-1)\n",
    "        we_min, we_max = float(env.observation_space[\"WE\"].start), float(env.observation_space[\"WE\"].start + env.observation_space[\"WE\"].n-1)\n",
    "        wt_min, wt_max = 0, env_info[\"max_wait_time\"]\n",
    "        pa_min, pa_max = 0, env.action_space.n-1\n",
    "        \n",
    "        queue_ns_scaled = (queue_ns-ns_min)*(self.num_tiles / (ns_max-ns_min))\n",
    "        queue_we_scaled = (queue_we-we_min)*(self.num_tiles / (we_max-we_min))\n",
    "        wt_scaled = (wt-wt_min)*(self.num_tiles / (wt_max-wt_min))\n",
    "        pa_scaled = (pa-pa_min)*(self.num_tiles / (pa_max-pa_min))\n",
    "\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [queue_ns_scaled, queue_we_scaled, wt_scaled, pa_scaled])\n",
    "        \n",
    "        return np.array(tiles)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SarsaAgent():\n",
    "    \"\"\"\n",
    "    Class for the Semi-Gradient Sarsa agent.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" All values are set to None so they can be initialized in the agent_init method \"\"\"\n",
    "        self.num_tilings = None\n",
    "        self.num_tiles = None\n",
    "        self.iht_size = None\n",
    "        self.initial_weights =None\n",
    "        self.action_count = None\n",
    "        \n",
    "        self.eps_start = None\n",
    "        self.eps_decay = None\n",
    "        self.eps_end = None\n",
    "        self.eps_runn = None\n",
    "        \n",
    "        self.discount = None\n",
    "        self.num_actions = None\n",
    "        self.step_size = None\n",
    "        self.w =None\n",
    "        self.tc = None\n",
    "        self.previous_tiles, self.previous_action = None, None\n",
    "\n",
    "    def info_init(self, info={}):\n",
    "        \"\"\"Setup for the agent, passed in a dictionary, called when the experiment first starts \"\"\"\n",
    "        self.num_tilings = info.get(\"num_tilings\", 8)\n",
    "        self.num_tiles = info.get(\"num_tiles\", 8)\n",
    "        self.iht_size = info.get(\"iht_size\", 4096)\n",
    "        self.initial_weights = info.get(\"initial_weights\", 0.0)\n",
    "\n",
    "        # epsilon\n",
    "        self.eps_start = info.get(\"eps_start\", 1.0)\n",
    "        self.eps_decay = info.get(\"eps_decay\", 2500)\n",
    "        self.eps_end = info.get(\"eps_end\", 1e-5)\n",
    "        self.eps_runn = info.get(\"eps_start\", 1.0)\n",
    "\n",
    "        self.discount = info.get(\"discount\", 1.0)\n",
    "        self.num_actions = info.get(\"num_actions\", 6)\n",
    "        self.step_size = info.get(\"step_size\", 0.5) / self.num_tilings\n",
    "\n",
    "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
    "\n",
    "        self.tc = TileCoder(iht_size=self.iht_size, \n",
    "                            num_tilings=self.num_tilings, \n",
    "                            num_tiles=self.num_tiles)\n",
    "        \n",
    "        self.action_count = defaultdict(int)\n",
    "        \n",
    "    def decay(self, episode):\n",
    "      \"\"\" \"\"\"\n",
    "      self.eps_runn = self.eps_end + (self.eps_start - self.eps_end)*np.exp(-1*episode/self.eps_decay)\n",
    "      \n",
    "    def get_freq_action(self):\n",
    "      tot_chosen = np.sum(list(self.action_count.values()))\n",
    "      return [freq_act/tot_chosen for _, freq_act in self.action_count.items()]\n",
    "\n",
    "    def get_value(self, state):\n",
    "      \"\"\" Function used to plot the estimates of state-action value \"\"\"\n",
    "      active_tiles = self.tc.get_tiles(queue_ns=state[\"NS\"], queue_we=state[\"WE\"], wt=state[\"wt\"], pa=state[\"pa\"]) \n",
    "      action, value = self.choose_action(active_tiles, greedy=True)\n",
    "      return action, value\n",
    "\n",
    "    def argmax(self, values):\n",
    "      top = float(\"-inf\")\n",
    "      ties = []\n",
    "      for i in range(len(values)):\n",
    "          if values[i] > top:\n",
    "              top = values[i]\n",
    "              ties = []\n",
    "          if values[i] == top:\n",
    "              ties.append(i)\n",
    "      return np.random.choice(ties)\n",
    "\n",
    "    def play(self, env, fin_score = 4000, render_=False, print_info=False):\n",
    "      ''' outside of training '''\n",
    "      state, info = env.reset()\n",
    "      if print_info:\n",
    "        print(\"The game has started...\")\n",
    "      while True:\n",
    "          active_tiles = self.tc.get_tiles(queue_ns=state[\"NS\"], queue_we=state[\"WE\"], wt=state[\"wt\"], pa=state[\"pa\"]) \n",
    "          action, _ = self.choose_action(active_tiles, greedy=True)\n",
    "          next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "          if render_:\n",
    "            # Render the game\n",
    "            env.render()\n",
    "          \n",
    "          if (done) or (info[\"score\"]>=fin_score): # If player is dead break\n",
    "            to_vis = info[\"score\"]\n",
    "            if print_info:\n",
    "              print(f\"\\nThe game is done! Final score: {to_vis: ,}\\n\")\n",
    "            break\n",
    "          else:\n",
    "            state = next_state  \n",
    "      env.close()\n",
    "      return to_vis\n",
    "\n",
    "    def choose_action(self, tiles, greedy=False):\n",
    "      ''' Function to choose action according to epsilon-greedy strategy,\n",
    "          based on current tile-based state representation  '''\n",
    "      action_values = [np.sum(self.w[action][tiles]) for action in range(self.num_actions)] \n",
    "      if (np.random.random()<self.eps_runn) and (not greedy):\n",
    "        chosen = np.random.choice(self.num_actions) \n",
    "      else:\n",
    "        chosen = self.argmax(action_values) \n",
    "      self.action_count[chosen] += 1\n",
    "      return chosen, action_values[chosen]\n",
    "\n",
    "    def start(self, state):\n",
    "      \"\"\" Take first move and store first action and tile-based representation of state \"\"\"\n",
    "      active_tiles = self.tc.get_tiles(queue_ns=state[\"NS\"], queue_we=state[\"WE\"], wt=state[\"wt\"], pa=state[\"pa\"]) \n",
    "      action, _ = self.choose_action(active_tiles, greedy=False)\n",
    "\n",
    "      self.previous_tiles = np.copy(active_tiles)\n",
    "      self.previous_action = action\n",
    "      return action\n",
    "\n",
    "    def update(self, reward, state):\n",
    "      \"\"\" Update of Sarsa algorithm (on-policy method)\n",
    "          The q-values of the previous state-action pair are updated\n",
    "          based on the value of the action taken in successive state (passed as parameter) \"\"\"\n",
    "\n",
    "      if state:\n",
    "        active_tiles = self.tc.get_tiles(queue_ns=state[\"NS\"], queue_we=state[\"WE\"], wt=state[\"wt\"], pa=state[\"pa\"]) \n",
    "        action, action_value = self.choose_action(active_tiles, greedy=False)\n",
    "      \n",
    "      action_value = action_value if state!=False else 0\n",
    "      update_target = reward + self.discount*action_value - np.sum(self.w[self.previous_action][self.previous_tiles])\n",
    "      self.w[self.previous_action][self.previous_tiles] += self.step_size * update_target * np.ones((self.num_tilings,))\n",
    "\n",
    "      if not state:\n",
    "        return\n",
    "      else:\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        self.previous_action = action\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loop_episodes(agent, env, env_steps, agent_info, change_reward=False, drop_epsilon=False, print_=False, print_info=False):\n",
    "    \"\"\" Function for the main loop of the Semi-Gradient Sarsa agent \"\"\"\n",
    "\n",
    "    num_episodes = agent_info.get(\"num_episodes\", 5000)  # agent initialization with info dictionary\n",
    "    agent.info_init(agent_info)\n",
    "\n",
    "    store, best_score = defaultdict(list), 0\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "      \n",
    "      # If drop_epsilon is an integer, training is stopped when episode number is equal to drop_epsilon\n",
    "      # A greedy game is then played in the environment and the final score is returned\n",
    "      #if (drop_epsilon!=False) and (i_episode==int(drop_epsilon)):\n",
    "      #  if print_info:\n",
    "      #    print(f\"Epsilon before evaluation: {agent.epsilon: .4f}, Last max score with epsilon: {total_max_scores[-1]}\")\n",
    "      #  fin_score = agent.play(env, fin_score=30000, print_=False, print_info=print_info)\n",
    "      #  return fin_score\n",
    "        \n",
    "      state, info = env.reset()   # first state\n",
    "\n",
    "      action = agent.start(state)  # first action\n",
    "      runn_score = 0\n",
    "      runn_rewards = []\n",
    "\n",
    "      while True:\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        runn_rewards.append(reward)\n",
    "\n",
    "        # we break out also when score is greater than a maximum cap\n",
    "        if (done) or (info[\"score\"]>30000):          \n",
    "          \n",
    "          agent.update(reward, False)  # update of one-to-last state\n",
    "\n",
    "          # update\n",
    "          best_score = max(best_score, info[\"score\"])\n",
    "          store[\"total_max_scores\"].append(best_score)\n",
    "          store[\"total_scores\"].append(info[\"score\"])\n",
    "          store[\"epsilon\"].append(agent.eps_runn)\n",
    "          store[\"avg_reward_episode\"].append(np.mean(runn_rewards))\n",
    "          break\n",
    "        \n",
    "        else:\n",
    "          action = agent.update(reward, next_state)\n",
    "          continue\n",
    "      \n",
    "        agent.decay()\n",
    "      \n",
    "      # info\n",
    "      action_freq = agent.get_freq_action()\n",
    "      store[\"action_freq\"].append(action_freq)\n",
    "      \n",
    "      if print_:\n",
    "        if i_episode % 1 == 0:\n",
    "          print(\"\\rEpisode {}/{}, Epsilon {} Avg Scores: {}, Max Score: {}, Action frequencies: {}.\".format(i_episode, num_episodes,\n",
    "                                                                         store[\"epsilon\"][-1], \n",
    "                                                                         np.mean(store[\"total_scores\"]).astype(int), \n",
    "                                                                         store[\"total_max_scores\"][-1],\n",
    "                                                                         \", \".join([str(round(x, 5)) for x in action_freq])))\n",
    "\n",
    "    return agent, store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/30, Epsilon 1.0 Avg Scores: 33, Max Score: 33, Action frequencies: 0.16667, 0.2, 0.1, 0.13333, 0.26667, 0.13333.\n",
      "Episode 2/30, Epsilon 1.0 Avg Scores: 28, Max Score: 33, Action frequencies: 0.23214, 0.17857, 0.125, 0.10714, 0.23214, 0.125.\n",
      "Episode 3/30, Epsilon 1.0 Avg Scores: 20, Max Score: 33, Action frequencies: 0.18182, 0.19481, 0.15584, 0.11688, 0.23377, 0.11688.\n",
      "Episode 4/30, Epsilon 1.0 Avg Scores: 15, Max Score: 33, Action frequencies: 0.17895, 0.2, 0.14737, 0.13684, 0.2, 0.13684.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\Agent_nico.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m info \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnum_tilings\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m8\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_tiles\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m8\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39miht_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4096\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meps_decay\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1800\u001b[39m,  \u001b[39m# with exponential decay\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39meps_end\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1e-6\u001b[39m} \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m agent_sarsa, store_sarsa \u001b[39m=\u001b[39m loop_episodes(agent_sarsa, env, env_steps, info, print_\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\Agent_nico.ipynb Cell 9\u001b[0m in \u001b[0;36mloop_episodes\u001b[1;34m(agent, env, env_steps, agent_info, change_reward, drop_epsilon, print_, print_info)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m runn_rewards \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   next_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   runn_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nicol/OneDrive/Desktop/M2/Reinforcement%20Learning/Final_project/traffic_mio_nuovo/Agent_nico.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   \u001b[39m# we break out also when score is greater than a maximum cap\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicol\\anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:38\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\traffic_control_game\\envs\\TrafficControl.py:170\u001b[0m, in \u001b[0;36mTrafficControlEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mmove_cars()  \n\u001b[0;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mcheck_lights()\n\u001b[1;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mstop_behind_car()\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mupdate_waiting()\n\u001b[0;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mupdate_score()\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\traffic_control_game\\envs\\logic.py:265\u001b[0m, in \u001b[0;36mGame.stop_behind_car\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m car \u001b[39min\u001b[39;00m cars:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m car\u001b[39m.\u001b[39mdriving:\n\u001b[1;32m--> 265\u001b[0m         cars_in_front \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mfilter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x: car\u001b[39m.\u001b[39;49mpos\u001b[39m.\u001b[39;49mdot(Point(car\u001b[39m.\u001b[39;49mmoving)) \u001b[39m<\u001b[39;49m x\u001b[39m.\u001b[39;49mpos\u001b[39m.\u001b[39;49mdot(Point(x\u001b[39m.\u001b[39;49mmoving)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcars_dict[\u001b[39mdir\u001b[39;49m]))\n\u001b[0;32m    267\u001b[0m         \u001b[39m# same road lane\u001b[39;00m\n\u001b[0;32m    268\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mdir\u001b[39m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mnorth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msouth\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\traffic_control_game\\envs\\logic.py:265\u001b[0m, in \u001b[0;36mGame.stop_behind_car.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m car \u001b[39min\u001b[39;00m cars:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m car\u001b[39m.\u001b[39mdriving:\n\u001b[1;32m--> 265\u001b[0m         cars_in_front \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m x: car\u001b[39m.\u001b[39mpos\u001b[39m.\u001b[39mdot(Point(car\u001b[39m.\u001b[39mmoving)) \u001b[39m<\u001b[39m x\u001b[39m.\u001b[39;49mpos\u001b[39m.\u001b[39;49mdot(Point(x\u001b[39m.\u001b[39;49mmoving)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcars_dict[\u001b[39mdir\u001b[39m]))\n\u001b[0;32m    267\u001b[0m         \u001b[39m# same road lane\u001b[39;00m\n\u001b[0;32m    268\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mdir\u001b[39m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mnorth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msouth\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive\\Desktop\\M2\\Reinforcement Learning\\Final_project\\traffic_mio_nuovo\\traffic_control_game\\envs\\logic.py:109\u001b[0m, in \u001b[0;36mPoint.dot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdot\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget())\u001b[39m.\u001b[39;49mdot(np\u001b[39m.\u001b[39;49marray(other\u001b[39m.\u001b[39;49mget()))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Environment initialization (no render for training)\n",
    "\n",
    "\n",
    "env_steps = 30\n",
    "max_waiting_time = 1500\n",
    "ps_ns = np.random.uniform(low=0.09, high=0.1, size=1)\n",
    "ps_ew = np.random.uniform(low=0.04, high=0.05, size=1)\n",
    "ps = np.tile(np.concatenate((ps_ns, ps_ew)), 2)\n",
    "\n",
    "env_info = {\"ps\": ps, \"max_wait_time\": max_waiting_time, \"env_steps\": env_steps, \"n_states\": 2}        \n",
    "        \n",
    "env = gym.make(\"traffic_control-v0\", env_info=env_info) \n",
    "\n",
    "agent_sarsa = SarsaAgent()\n",
    "num_episodes = 30 #5000\n",
    "\n",
    "\n",
    "info = {\"num_tilings\": 8,\n",
    "        \"num_tiles\": 8,\n",
    "        \"iht_size\": 4096,\n",
    "        \"num_episodes\": num_episodes,\n",
    "        \"num_actions\": env.action_space.n,\n",
    "        \"step_size\": 0.5,  # divided by number of tilings\n",
    "        \"discount\": 1.0,\n",
    "        \"eps_start\": 1.0,\n",
    "        \"eps_decay\": 1800,  # with exponential decay\n",
    "        \"eps_end\": 1e-6} \n",
    "\n",
    "\n",
    "start = timer()\n",
    "\n",
    "agent_sarsa, store_sarsa = loop_episodes(agent_sarsa, env, env_steps, info, print_=True)\n",
    "\n",
    "# Training\n",
    "\n",
    "print(\"\")\n",
    "timer(start, \"Training Sarsa agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(store_sarsa, \"store_sarsa\")\n",
    "\n",
    "with open('q_table_sarsa.npy', 'wb') as f:\n",
    "  np.save(f, agent_sarsa.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot evolution scores (running mean), together with maximum score\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "titles = [\"Score evolution\", \"Max score evolution\", \"Epsilon evolution\"]\n",
    "\n",
    "total_scores, total_max_scores, eps = store_sarsa[\"total_max_scores\"], store_sarsa[\"total_scores\"], store_sarsa[\"epsilon\"]\n",
    "\n",
    "for idx, (ax, score, title) in enumerate(zip(axs, [total_scores, total_max_scores, eps], titles)):\n",
    "  if idx==0:\n",
    "    ax.plot(score, linewidth=1.2)\n",
    "    means = np.lib.stride_tricks.sliding_window_view(score, 15).mean(1).reshape(-1)\n",
    "    ax.plot(means, linewidth=2.5)\n",
    "  else:\n",
    "    ax.plot(score, linewidth=1.2)\n",
    "  ax.set_xlabel('Episode', fontsize=14)\n",
    "  ax.set_title(title, fontsize=15)\n",
    "\n",
    "plt.suptitle(\"Semi-gradient Sarsa agent\", fontsize=22)\n",
    "\n",
    "#plt.savefig('evolution_sarsa.jpg', bbox_inches='tight', dpi=300)   \n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Play a game\n",
    "\n",
    "env_steps = 500\n",
    "\n",
    "env = gym.make(\"traffic_control-v0\", n_states = 2, env_steps = env_steps, render_mode=\"human\") \n",
    "\n",
    "agent_sarsa.play(env, fin_score = 4000, render_=True, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "\n",
    "#actions_loop = [0]*5 + [1]*5 + [0] + [1]*5 + [0] + [1]*5\n",
    "#actions_loop = [2]*10+[0]\n",
    "actions_loop = [1]*10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
