{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import time\n",
    "import sys\n",
    "import traffic_control_game\n",
    "import pygame\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import json\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import matplotlib\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import tiles3 as tc\n",
    "\n",
    "#is_ipython = 'inline' in matplotlib.get_backend()\n",
    "#if is_ipython:\n",
    "#    from IPython import display\n",
    "\n",
    "#plt.ion()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None, string=None):\n",
    "    '''\n",
    "    Function to compute the time\n",
    "    start_time : starting time generated calling this function without arguments the first time\n",
    "    string: visualization purposes (task description)\n",
    "    '''\n",
    "    if not start_time:\n",
    "        start_time=datetime.datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.datetime.now()-start_time).total_seconds(),3600)\n",
    "        tmin, tsec = divmod(temp_sec,60)\n",
    "        pr = \"for \" + str(string) + \" \" if string else \"\"\n",
    "        print(\"Execution time\", pr, \"is \", thour,\" h :\", tmin,' m :', round(tsec,2), \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to load and store dictionaries safely by saving converting them\n",
    "to json format. Used due to the long training times and the instability\n",
    "of Collab\n",
    "\"\"\"\n",
    "\n",
    "def save_json(q, filename):\n",
    "    with open(f'{filename}.json', 'w') as fp:\n",
    "        to_json = str({k: list(v) for k, v in q.items()})\n",
    "        json.dump(to_json, fp)\n",
    "    \n",
    "\n",
    "def load_json(filename):\n",
    "    with open(f'{filename}.json') as json_file:\n",
    "        data = ast.literal_eval(json.load(json_file))\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations space: Dict('NS': Discrete(76), 'WE': Discrete(92))\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Environment initialization\n",
    "\n",
    "env = gym.make(\"traffic_control-v0\", n_states = 2, render_mode=\"human\") \n",
    "\n",
    "print(f\"Observations space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('NS', 32), ('WE', 57)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('NS', 10), ('WE', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.observation_space.sample()\n",
    "#obs = np.array(list(a.values()))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 75.0 0.0 91.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-gradient Sarsa agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TileCoder:\n",
    "    ''' Class to facilitate tile coding representations of states passed as parameters '''\n",
    "    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n",
    "        self.iht = tc.IHT(iht_size)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.num_tiles = num_tiles\n",
    "    \n",
    "    def get_tiles(self, queue_ns, queue_we):\n",
    "        # Range of minimum and maximum value for each of the 2 components of the observation vector\n",
    "        x_min, x_max = float(env.observation_space[\"NS\"].start), float(env.observation_space[\"NS\"].start + env.observation_space[\"NS\"].n-1)\n",
    "        y_min, y_max = float(env.observation_space[\"WE\"].start), float(env.observation_space[\"WE\"].start + env.observation_space[\"WE\"].n-1)\n",
    "        \n",
    "        queue_ns_scaled = (queue_ns-x_min)*(self.num_tiles / (x_max-x_min))\n",
    "        queue_we_scaled = (queue_we-y_min)*(self.num_tiles / (y_max-y_min))\n",
    "\n",
    "        tiles = tc.tiles(self.iht, self.num_tilings, [queue_ns_scaled, queue_we_scaled])\n",
    "        \n",
    "        return np.array(tiles)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = list(np.arange(10000))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.plot(a, a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SarsaAgent():\n",
    "    \"\"\"\n",
    "    Class for the Semi-Gradient Sarsa agent.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" All values are set to None so they can be initialized in the agent_init method \"\"\"\n",
    "        self.num_tilings = None\n",
    "        self.num_tiles = None\n",
    "        self.iht_size = None\n",
    "        self.initial_weights =None\n",
    "        self.epsilon = None\n",
    "        self.epsilon_decay = None\n",
    "        self.epsilon_min = None\n",
    "        self.discount = None\n",
    "        self.num_actions = None\n",
    "        self.step_size = None\n",
    "        self.w =None\n",
    "        self.tc = None\n",
    "        self.previous_tiles, self.previous_action = None, None\n",
    "\n",
    "    def info_init(self, info={}):\n",
    "        \"\"\"Setup for the agent, passed in a dictionary, called when the experiment first starts \"\"\"\n",
    "        self.num_tilings = info.get(\"num_tilings\", 8)\n",
    "        self.num_tiles = info.get(\"num_tiles\", 8)\n",
    "        self.iht_size = info.get(\"iht_size\", 4096)\n",
    "        self.initial_weights = info.get(\"initial_weights\", 0.0)\n",
    "\n",
    "        # EPSILON????????\n",
    "        self.epsilon = info.get(\"eps_start\", 1.0)\n",
    "        self.epsilon_decay = info.get(\"eps_decay\", .99999)\n",
    "        self.epsilon_min = info.get(\"eps_min\", 0.05)\n",
    "\n",
    "        self.discount = info.get(\"discount\", 1.0)\n",
    "        self.num_actions = info.get(\"num_actions\", 6)\n",
    "        self.step_size = info.get(\"step_size\", 0.5) / self.num_tilings\n",
    "\n",
    "        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n",
    "\n",
    "        self.tc = TileCoder(iht_size=self.iht_size, \n",
    "                            num_tilings=self.num_tilings, \n",
    "                            num_tiles=self.num_tiles)\n",
    "        \n",
    "    def decay(self):\n",
    "      \"\"\" Classical geometric decay of epsilon as beseline \"\"\"\n",
    "      self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def get_value(self, state):\n",
    "      \"\"\" Function used to plot the estimates of state-action value \"\"\"\n",
    "      active_tiles = self.tc.get_tiles(x_distance=state[0], y_distance=state[1]) \n",
    "      action, value = self.choose_action(active_tiles, greedy=True)\n",
    "      return action, value\n",
    "\n",
    "    def argmax(self, values):\n",
    "      if values[0]==values[1]:\n",
    "        return np.random.choice(self.num_actions)\n",
    "      else:\n",
    "        return np.argmax(values)\n",
    "\n",
    "    def play(self, env, fin_score = 30000, print_=False, print_info=False):\n",
    "      ''' outside of training '''\n",
    "      state, info = env.reset()\n",
    "      if print_info:\n",
    "        print(\"The game has started...\")\n",
    "      while True:\n",
    "          active_tiles = self.tc.get_tiles(x_distance=state[0], y_distance=state[1]) \n",
    "          action, _ = self.choose_action(active_tiles, greedy=True)\n",
    "          next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "          if print_:\n",
    "            # Render the game\n",
    "            os.system(\"clear\")\n",
    "            sys.stdout.write(env.render())\n",
    "            time.sleep(0.2) # FPS\n",
    "          \n",
    "          if (done) or (info[\"score\"]>=fin_score): # If player is dead break\n",
    "            to_vis = info[\"score\"]\n",
    "            if print_info:\n",
    "              print(f\"\\nThe game is done! Final score: {to_vis: ,}\\n\")\n",
    "            break\n",
    "          else:\n",
    "            state = next_state  \n",
    "      env.close()\n",
    "      return to_vis\n",
    "\n",
    "    def choose_action(self, tiles, greedy=False):\n",
    "      ''' Function to choose action according to epsilon-greedy strategy,\n",
    "          based on current tile-based state representation  '''\n",
    "      action_values = [np.sum(self.w[action][tiles]) for action in range(self.num_actions)] \n",
    "      if (np.random.random()<self.epsilon) and (not greedy):\n",
    "        chosen = np.random.choice(self.num_actions) \n",
    "      else:\n",
    "        chosen = self.argmax(action_values) \n",
    "      return chosen, action_values[chosen]\n",
    "\n",
    "    def start(self, state):\n",
    "      \"\"\" Take first move and store first action and tile-based representation of state \"\"\"\n",
    "      active_tiles = self.tc.get_tiles(x_distance=state[0], y_distance=state[1]) \n",
    "      action, _ = self.choose_action(active_tiles, greedy=False)\n",
    "\n",
    "      self.previous_tiles = np.copy(active_tiles)\n",
    "      self.previous_action = action\n",
    "      return action\n",
    "\n",
    "    def update(self, reward, state):\n",
    "      \"\"\" Update of Sarsa algorithm (on-policy method)\n",
    "          The q-values of the previous state-action pair are updated\n",
    "          based on the value of the action taken in successive state (passed as parameter) \"\"\"\n",
    "\n",
    "      if state:\n",
    "        active_tiles = self.tc.get_tiles(x_distance=state[0], y_distance=state[1]) \n",
    "        action, action_value = self.choose_action(active_tiles, greedy=False)\n",
    "      \n",
    "      action_value = action_value if state!=False else 0\n",
    "      update_target = reward + self.discount*action_value - np.sum(self.w[self.previous_action][self.previous_tiles])\n",
    "      self.w[self.previous_action][self.previous_tiles] += self.step_size * update_target * np.ones((self.num_tilings,))\n",
    "\n",
    "      if not state:\n",
    "        return\n",
    "      else:\n",
    "        self.previous_tiles = np.copy(active_tiles)\n",
    "        self.previous_action = action\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "\n",
    "#actions_loop = [0]*5 + [1]*5 + [0] + [1]*5 + [0] + [1]*5\n",
    "#actions_loop = [2]*10+[0]\n",
    "actions_loop = [1]*10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
